* neuroGrape - an attempt to approximate the evaluation function of
* the Grapefruit chess engine with a backpropagation neural network.

Grapefruit is an open source chess engine using the UCI
protocol[fn:1].  Grapefruit is derivative of Toga which is derivative
of Fruit.  Information for these engines and their modifications can
be found at
http://www.computerchess.info/tdbb/phpBB3/viewforum.php?f=9

* grapeLog - http://github.com/m00natic/grapeLog
is a slight modification of Grapefruit which logs its eval over
positions in FEN notation[fn:2].  There must be `logs' directory in
the engine's folder where evals will be logged.  This information is
used for learning.

* nTrain (neuroTrainer) - http://github.com/m00natic/grapeTrainer
is a little GUI manipulating backpropagation neural networks.  `Train
directory' specifies directory of log files to be used.  `Convert
train files' converts raw .log files to .bpnl files where FEN notation
is recoded in 262 bits + eval number allowing slightly faster training
and assessment.  This conversion is optional and doesn't seem to give
substantial advantage.  If there is even one .bpnl file in the `Train
directory', only .bpnl files will be used for training or assessment.
Otherwise .log files will be used.

During start a neural network is created with the following
characteristics: 4 layers - input with 262 neurons, hidden with 66
neurons, hidden with 256 neurons and output with 1 neuron.  Neurons
from all layers except the input have bias.  All neurons have a
sigmoid output function ranging in (-1; 1).  Network output is scaled
by 29744 which is Grapefruit's internal maximum.  Learning rate is
0.35, acceleration 0.3.  `Save' writes current network to file which
can later be loaded through the `Load' button.  Structure of these
files fully specifies the neural network.  The first line describes
main characteristics and the rest - weights of all neurons.  Elements
from the first line specify respectively:
1) absolute value of range where random numbers will be used for
   initializing weights.
2) learning rate
3) acceleration
4) output scaling constant
5) total number of layers including input and output
6) sequentially enumerating characteristics of the layers
   - number of neurons
   - is there bias
   - constant which specifies output function.  For now functions are:
     0 - linear
     1 - sigmoid with range (0; 1)
     2 - sigmoid with range (-1; 1)
7) if the line ends with `x' - initialization of all weights will be
   done.  This is a way of creating new networks with different
   characteristics than the default.  Weights will be initialized
   after `Load' of the file.

`Train' button invokes backpropagation learning over `Train directory'
where each training file is used `Epochs' number of times.  For just
in case, in bpn~ file current state of the network is backup-ed after
each file is iterated.  On error, network state is restored from
there.  `Test' calculates mean error which the current network gives
over the files in `Train directory' and total mean error.

* neuroGrape - http://github.com/m00natic/neuroGrape
is a modification of Grapefruit that uses neural network created with
nTrain for evaluating positions.  For now, name of the neural network
file is fixed to `grape.bpn' and has to reside within the same
directory.

All 3 programmes are written in C++ with nTrain being a Qt[fn:5] app.
Instructions for running under GNU/Linux: for example using
XBoard[fn:3] along with UCI adaptor Polyglot[fn:4].  There are example
.ini configuration files and a shell script for arranging engine
tournaments in the download section.


* Footnotes
[fn:1] http://en.wikipedia.org/wiki/Universal_Chess_Interface
[fn:2] http://en.wikipedia.org/wiki/Forsyth-Edwards_Notation
[fn:3] http://www.tim-mann.org/xboard.html
[fn:4] http://wbec-ridderkerk.nl/html/details1/PolyGlot.html
[fn:5] http://www.qtsoftware.com
